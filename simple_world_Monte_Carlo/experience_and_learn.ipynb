{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Control code for simple world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "\n",
    "* 1.1 inputs and the random seed\n",
    "\n",
    "* 1.2 grid size along each direction\n",
    "\n",
    "* 1.3 number of actions: \n",
    " if you are going to change it, you should also\n",
    " change the pointwise action function in RL_utils as well\n",
    "\n",
    "* 1.4 discount should be set to a value smaller than 1\n",
    "\n",
    "\n",
    "* 1.5 number of learning episodes\n",
    "\n",
    "\n",
    "* 1.6 epsilon for the epsilon greedy\n",
    "* 1.7 initializing the policy $\\pi$ to a random policy\n",
    "* 1.8 initializing all $Q(s, a)$ to zero\n",
    "* 1.9 setting up the plot\n",
    "\n",
    "# 2. policy iteration\n",
    "\n",
    "## 2.1 policy evaluation loops\n",
    "\n",
    "For infinite number of trials\n",
    "\n",
    "* Choose a random pair of $(s, a)$. Lets call it $(s_0, a_0)$\n",
    "* Start the episode with $(s_0, a_0)$, follow $\\pi(a|s)$ until you reach the terminal state. Estimate $Q(s_0, a_0$.\n",
    "* Using the estimate of $Q(s_0, a_0)$ and all its previous estimates, update the Q\n",
    "\n",
    "# 2.2 updating the policy using Q values (to epsilon greedy)\n",
    "Eaaaasy!\n",
    "# 2.3 visual monitoring porpuses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from plot_utils import create_plot, plotter\n",
    "from RL_utils import return_a_random_policy\n",
    "from RL_utils import step\n",
    "from RL_utils import choose_an_action_based_on_pi\n",
    "from RL_utils import return_epsilon_greedy_pi\n",
    "from RL_utils import initialize_the_state\n",
    "from RL_utils import Bellmann_iteration\n",
    "\n",
    "# 1. Initialization\n",
    "\n",
    "# 1.1 inputs and the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# 1.2 grid size along each direction\n",
    "n = 8\n",
    "\n",
    "# 1.3 nr of actions\n",
    "# if you are going to change it:\n",
    "# change the pointwise action function in RL_utils as well\n",
    "nr_actions = 4\n",
    "\n",
    "# 1.4 discount\n",
    "# should be set to a value smaller than 1\n",
    "gamma = 0.98\n",
    "\n",
    "# 1.5 nr of learning episodes\n",
    "nr_episodes = 10_000\n",
    "\n",
    "# 1.6 epsilon for the epsilon greedy\n",
    "epsilon = 0.4\n",
    "\n",
    "# 1.7 policy $\\pi$\n",
    "# initializing policy to a random policy\n",
    "# initializing Q to zero\n",
    "\n",
    "pi = return_a_random_policy(n, nr_actions)\n",
    "Q_accumulate = np.zeros((n, n, nr_actions))\n",
    "Q_visit_counter = np.zeros((n, n, nr_actions))\n",
    "\n",
    "# 1.8 setting up the plot\n",
    "ax = create_plot(n)\n",
    "plt.ion()\n",
    "interactive(True)\n",
    "plt.cla()\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "# 2. policy iteration\n",
    "for episode_id in range(nr_episodes):\n",
    "    print(episode_id)\n",
    "\n",
    "    # 2.1 policy evaluation loops\n",
    "    # Exercise:\n",
    "    # The following function might be useful:\n",
    "    # initialize_the_state(n)\n",
    "    # choose_an_action_based_on_pi(s, pi)\n",
    "    # step(s0, a0, n)\n",
    "    \n",
    "    # 2.2 updating the policy using Q values (to epsilon greedy)\n",
    "    pi = return_epsilon_greedy_pi(Q, epsilon)\n",
    "\n",
    "    v = np.zeros(shape=(n, n))\n",
    "\n",
    "    # 2.3 visual monitoring porpuses\n",
    "    for i in range(100):\n",
    "        v = Bellmann_iteration(np.argmax(pi, axis=-1), v, gamma)\n",
    "    plotter(ax, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
