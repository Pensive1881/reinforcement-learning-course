{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration for a very simple world!\n",
    "\n",
    "In this code the optimal policy is found for:\n",
    "for an agent finding the path to its destination.\n",
    "\n",
    "*Dynamic programming* is used find the optimal values and policy with the notations in chapter 4 of THE book.\n",
    "The book: Reinforcement learning (introduction) by Sutton & Batto, second edition.\n",
    "\n",
    "# The code is structured as:\n",
    "* 1. initialization\n",
    "* 2. policy evaluation, just for fun of it!\n",
    "* 3. policy iteration\n",
    "* 4. printing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plot_utils import create_plot, plotter\n",
    "from RL_library import return_pointwise_A\n",
    "from RL_library import Bellmann_iteration\n",
    "from RL_library import Q_estimate\n",
    "from RL_library import simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "\n",
    "* 1.1 initializing the random seed and the grid size along each direction\n",
    "\n",
    "* 1.2 policy $\\pi$: setting policy to a random policy\n",
    "\n",
    "* 1.3 values, $v(s)$ setting all to zero\n",
    "\n",
    "* 1.4 discount $\\gamma$ which should be set to a value smaller than 1\n",
    "\n",
    "* 1.5 setting up the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 inputs and the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# grid size along each direction\n",
    "n = 10\n",
    "\n",
    "# 1.2 policy $\\pi$\n",
    "# setting policy to a random policy\n",
    "pi = np.random.random_integers(low=0, high=4, size=(n, n))\n",
    "print(pi)\n",
    "\n",
    "# 1.3 values, v\n",
    "# setting all to zero\n",
    "v = np.zeros(shape=(n, n))\n",
    "\n",
    "# 1.4 discount\n",
    "# should be set to a value smaller than 1\n",
    "gamma = 0.99\n",
    "\n",
    "# 1.5 setting up the plot\n",
    "ax = create_plot(n)\n",
    "plt.ion()\n",
    "interactive(True)\n",
    "plt.cla()\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. policy evaluation #\n",
    "Eq. (4.5) of the book.\n",
    "\n",
    "At this point we can only evaluate the initial policy which is the random policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. policy evaluation #\n",
    "niteration = 10\n",
    "\n",
    "for iteration in range(0, niteration):\n",
    "    v = Bellmann_iteration(pi, v, gamma)\n",
    "    plotter(ax, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. policy iteration\n",
    "\n",
    "* 3.1 setting the number of iterations for the policy improvement. This can be replaced by a measure of convergence\n",
    "* 3.2 initializing values to zero.\n",
    "\n",
    "* 3.3 the main iterative loop\n",
    " Eq. (4.7) of the book:\n",
    " each step is the Bellman operation for policy evaluation followed by a policy improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. policy iteration\n",
    "\n",
    "# 3.1 number of iterations for the policy improvement\n",
    "# this can be replaced by a measure of convergence\n",
    "nr_iterations = 100\n",
    "\n",
    "# 3.2 initializing values\n",
    "# values are set to zero.\n",
    "v = np.zeros(shape=(n, n))\n",
    "\n",
    "# 3.3 the main iterative loop\n",
    "# Eq. (4.7) of the book:\n",
    "# each step is the Bellman operation for policy evaluation\n",
    "# followed by a policy improvement\n",
    "step = 0\n",
    "\n",
    "while step < nr_iterations:\n",
    "    new_pi = np.zeros(shape=(n, n))\n",
    "    # policy evaluation\n",
    "    v = Bellmann_iteration(pi, v, gamma)\n",
    "    # policy improvement\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            # Exercise\n",
    "            # iterate over all at point A[i, j] to find the best action, i.e. the largest Q(s, a)\n",
    "            # to this end you can use the following functions:\n",
    "            # return_pointwise_A and\n",
    "            # Q_estimate\n",
    "            # when you found the best action, set the new policy (new_pi) to that action\n",
    "    pi = new_pi + 0.0\n",
    "    plotter(ax, v)\n",
    "    step += 1\n",
    "\n",
    "    if (step % 100 == 1):\n",
    "        print(\"#iteration: \" + str(step - 1))\n",
    "\n",
    "\n",
    "simulate(4, 5, pi)\n",
    "simulate(4, 5, pi, color='gray', nr_actions=5, randomize=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. printing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(v)\n",
    "plt.show()\n",
    "np.savetxt('optimal_pi.dat', pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
